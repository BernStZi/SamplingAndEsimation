%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[10pt]{beamer}
\usepackage{etex}

\usepackage[T1]{fontenc}
\usepackage{textpos} 
\usepackage{hyperref}
\usepackage{amsmath,amsthm,amsfonts,nicefrac,mathabx,amssymb,amsbsy}
 \usepackage[subnum]{cases}
\usepackage{calligra, mathrsfs}
%\usepackage{natbib}
\usepackage{booktabs}
%\bibpunct{(}{)}{;}{a}{,}{,}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{helvet}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow,dcolumn}
\usepackage{listings}
\usepackage{ragged2e}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{animate} %for animated graphs
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{url}
\usepackage{bibentry}
\usepackage{chngcntr}

\ifx\hypersetup\undefined
  \AtBeginDocument{%
    \hypersetup{unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=false}
  }
\else
  \hypersetup{unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=false}
\fi

\colorlet{tablesubheadcolor}{gray!25}
\colorlet{tableheadcolor}{gray!40}
\colorlet{tablerowcolor}{gray!15.0}
\usetheme{Gesis}
%\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]%{\hspace*{.2cm}\insertframenumber}
\setbeamerfont{caption}{size=\footnotesize}
\usefonttheme[onlylarge]{structuresmallcapsserif} % alte Schrift


\definecolor{hellgrau}{rgb}   {0.109375,  0.40625,   0.51953125}
\definecolor{dunkelgrau}{rgb} {0.009375,  0.30625,   0.41953125}
\definecolor{dunkelgrau2}{rgb}{0.009375,  0.20625,   0.31953125}
\definecolor{hellbraun}{rgb}  {0.9140625, 0.8984375, 0.8046875}
\definecolor{hellbraun2}{rgb} {.95,       0.9,       0.8}
\definecolor{alertred}{rgb}   {0.8515625, 0.3828125, 0.08984375}
\definecolor{orange}{rgb}{1,0.5,0}


\setbeamercolor{firstsecslide}{fg=white,bg=dunkelgrau}
\setbeamertemplate{blocks}[rounded][shadow=true]

\newcolumntype{d}[1]{D{.}{.}{#1}}

\newcommand{\emphred}[1]{\textcolor{alertred}{#1}}
\newcommand{\emphcol}[1]{\textcolor{dunkelgrau}{\slshape #1}}

\newcommand{\eqname}[1]{\tag*{#1}} %equation title

\newenvironment{frcseries}{\fontfamily{frc}\selectfont}{}
\newcommand{\textfrc}[1]{{\frcseries#1}}
\newcommand{\mathfrc}[1]{\text{\textfrc{#1}}}


\setcounter{tocdepth}{1}
\setbeamercolor*{section in toc}{fg=hellgrau}
\setbeamertemplate{bibliography item}[default]


\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\newsavebox{\mysavebox}


\newcommand{\E}[1]{\text{E}\left(#1\right)}
\newcommand{\V}[1]{\text{V}\left(#1\right)}
\newcommand{\Vest}[1]{\widehat{\text{V}}\left(#1\right)}
\newcommand{\MSE}[1]{\text{MSE}\left(#1\right)}
\newcommand{\COV}[2]{\text{COV}\left(#1,\,#2\right)}
\newcommand{\deff}{\ensuremath{\text{\slshape deff}}}
%roman numbers in equation
\newcommand{\RN}[1]{%
  \textup{\uppercase\expandafter{\romannumeral#1}}%
}


\makeatletter

\addtobeamertemplate{frametitle}{}{%
\begin{textblock*}{100mm}(.91\textwidth,-1cm)
\includegraphics[height=1cm,width=2cm]{graphs/logos/GESIS_Logo_kompakt_en.jpg}
\end{textblock*}}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
% this default might be overridden by plain title style
 \newcommand\makebeamertitle{\frame{\maketitle}}%
 % (ERT) argument for the TOC
 \AtBeginDocument{%
   \let\origtableofcontents=\tableofcontents
   \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
   \def\gobbletableofcontents#1{\origtableofcontents}
 }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\usetheme{Montpellier}%\usetheme{PaloAlto}

\makeatother

\begin{document}
<<setup, include=FALSE>>=
library(knitr)
library(xtable)
library(plyr)
library(survey)
library(sampling)
library(mvtnorm)
library(PracTools)
library(GGally)
library(ggplot2)
opts_chunk$set(fig.path='graphs/beamer-',fig.align='center',fig.show='hold',size='footnotesize')
@


\title[Estimation]{Sampling and Estimation}   
\subtitle{Day 3: Estimation in Complex Survey Designs}

\author{Stefan Zins\thanks{\href{mailto:Stefan.Zins@gesis.org}{Stefan.Zins@gesis.org}} and Matthias Sand\thanks{\href{mailto:Matthias.Sand@gesis.org}{Matthias.Sand@gesis.org}}}
\date{\today} 

\makebeamertitle

\begin{frame}[fragile]{Survey Imperfections}
\begin{figure}[H]
\begin{minipage}[c]{1\textwidth}
%            \label{Missings}
                \centering
%\noindent{\caption{\small{\textbf{Stichprobenziehung}}}}
\vspace{.25cm}
\begin{tikzpicture}[scale=1, fill opacity=0.6,font=\normalsize]
\draw[black,line width=2.5pt] (0,0) -- ++(0:6cm) -- ++(90:4cm) -- ++(180:6cm) -- ++(270:4cm);
\draw[black,thick] (-1.25,1.25) -- ++(0:6cm) -- ++(90:4cm) -- ++(180:6cm) -- ++(270:4cm);
\draw[ draw = black,thick] (2.5,3.25) circle (1.75);
\draw[fill=black!50, draw = black,thick] (2.5,3.25) circle (1.25);

\coordinate[label=right:Target Population] (A) at (7,2);
\draw (6,2) -- (A);
\coordinate[label=right:Sampling Frame] (B) at (5.75,4.75);
\draw (4.75,4.75) -- (B);
\coordinate[label=left:Full Sample] (C) at (0.5,5.75);
\draw (1.62,4.725) -- (C);
\coordinate[label=right:Non-respondents] (D) at (3.5,5.75);
\draw (3.25,4.6) -- (D);
\coordinate[label=right:Respondents] (E) at (5.75,5.25);
\draw (3.7,3.5) -- (E);
\coordinate[label=left:Under-coverage] (F) at (2.75,0.5);
\coordinate[label=above:\rotatebox{90}{Over-coverage}] (G) at (-1,1.35);
\end{tikzpicture}
%\centering\small{\textbf{Angaben in Prozent}}
\end{minipage}
\end{figure}
\end{frame}

% 
\frame{
  \frametitle{Methods to Handle Missing Data}

\begin{itemize}
 \item<1->  Procedures based on the {\bf available cases} only, i.e., only those cases that are completely
 recorded for the variables of interest.
 \item<2-|alert@5> {\bf Weighting procedures} that adjust design weights to compensate the bias that a MAR non-response might inflict on HT-type estimators.
 \item<3-> {\bf Single imputation} and correction of the variance estimates to
 account for imputation uncertainty.
 \item<4-> {\bf Multiple imputation} (MI) according to Rubin (1978, 1987).
%  \item {\bf Model-based corrections} of parameter estimates such as the
%  expectation-maximization (EM) algorithm
 \end{itemize}

\onslide+<5>{$\boldsymbol{\longrightarrow}$\quad {Methods for handling coverage errors are not so widely spread, simply because there is often no reliable auxiliary information on just the target population. However if there is, it can receive a treatment similar to that of weighting by non-response.}
}
}


\begin{frame}{Missing Data} 
\emph{Missing data is the norm, rather than the expectation!}

%\begin{minipage}[h]{8cm}
% \begin{center}
Missingness may be either
\begin{description} 
  \item[MCAR]<2-> missing completely at random,  
   \begin{itemize}
     \item every unit has same response propensity (RP)
     \item respondents are a random sample of the initial sample
   \end{itemize}
  \item[MAR]<3->  missing at random, or
   \begin{itemize}
     \item RP depends on auxiliary variables $\boldsymbol{\mathcal{X}}$
     \item can be modeled, if $\boldsymbol{\mathcal{X}}$ is known for both respondents \& non-respondents
   \end{itemize}
  \item[MNAR]<4-> missing not at random \vspace{2mm}
  \begin{itemize}
    \item RP depends on variables of interest $\mathcal{Y}$ 
    \item cannot be modeled, because $\mathcal{Y}$ not known for non-respondents
  \end{itemize}
\end{description}
\uncover<4>{[Rubin and Little 2002]
%\end{center}\vspace*{3mm}
%\end{minipage}
~\\[3mm]
$\boldsymbol{\longrightarrow}$\quad {In
multivariate analysis often 30\% to 40\% of the data are lost with
case deletion assuming MCAR!}
}
\end{frame}



\section{Weighting Methods}

 \begin{frame}{Weighting Methods}

\begin{description}
\item[Calibration approach]<1|only@1> The design weighs are calibrated to the totals of some auxiliary variables $\boldsymbol{\mathcal{X}}$.
\begin{itemize}
  \item Sample estimates using the calibrated weights will exactly replicated those totals.
  \item If the used auxiliary variables help to explain the response process the calibrated weight can reduce the non-response error.
\end{itemize}
\item[Two-phase approach]<2|only@2> The response process is modeled to obtain the response propensities $\psi_k$ for all $k \in \mathfrc{s}$. The new weight of element $k$ is $\frac{d_k}{\psi_k}$. (Two phases: 1. Sampling $\boldsymbol{\rightarrow}$ 2. Responding). 
\item<2|only@2> In addition the new weights $\frac{d_k}{\psi_k}$ might then also be calibrated. 
\item<2|only@2> Often used models are: 
\begin{itemize}
  \item Response homogeneity classes, every element in a class has the same probability to respond.
  \item Generalized liner models (\emph{probit}, \emph{logit}, \emph{log-log}), treating response as a latent variable.
\end{itemize}
\end{description}
\onslide+<3>{
The calibration approach is more direct as the design weights are directly calibrated without considering the response propensities.
Also, if the same models are used for both the modeling of the response propensities and the calibration the two approaches can be equivalent.
}
\end{frame}



\begin{frame}{Weights}
\onslide*<1-2>{
Generic estimators for a total and  a mean
$$\hat{\tau}_w  = \sum_{k \in \mathfrc{s}} w_k y_k \quad\text{and}\quad \overline{y}_w  = \dfrac{\sum_{k \in \mathfrc{s}} w_k y_k}{ \sum_{k \in \mathfrc{s}} w_k }\;{,}$$
where $w_k$ is the survey weight of element $k$, with
}\onslide<2->{
$$
w_k =
	\begin{cases}
	d_k  g_k & \text{for}\; k \in \mathfrc{s} \\
	0          & \text{else}
	\end{cases}\,{.}
$$
}\onslide<3>{Sometimes  called base weights or design weights, the inverse of inclusion probabilities $d_k=\pi^{-1}$ is usually the first step in weighting. If we have $g_k=1$ the $\hat{\tau}_w$ would be the HT estimator or $\pi$-estimator.
The factor $g_k$ adjusts the design weights to reduce
\begin{itemize}
\item the sampling error (i.e. variance),
\item the non-response error, and
\item the coverage error 
\end{itemize}
of estimator $\hat{\tau}_w$ or $\overline{y}_w$.
Thereby the $w_k$'s should not deviate to much from the $d_k$'s as these weights ensure an unbiased estimation.
}
\end{frame}

%in any case we need good auxiliary infomation dealt with non-response or coverage errors.
%varaible that are either observered for the whole sample or even the whole population.

\begin{frame}{Weights Calibration I}

\onslide*<1>{The general idea is to exploit the relationship between auxiliary variables and the variable of interest to improve the efficiency of estimators.}
\onslide*<2>{ The following problem is solved with weight calibration: \newline
For a give design $p(.)$ and a sample $\mathfrc{s}$ weights $w_k$ for all $k \in  \mathfrc{s}$ have to be found that minimize
 $$\sum_{k \in \mathfrc{s} } G_k(w_k,d_k,c_k)\;,$$
subject to constraints
 $$
 \sum_{k \in \mathfrc{s} } w_k \mathbf{x}_k = \sum_{k \in \mathcal{U} } \mathbf{x}_k = \boldsymbol{\tau}_x
 $$
where $\mathbf{x}_k=(x_{k1},\,x_{k2},\,\ldots,\,x_{kQ})^\top$ is a vector of $q$ auxiliary variables for element $k$. $G_k$ is a measure of distance between $w_k$ and $d_k$ and $c_k$ is a factor that can be freely chosen for additional flexibility.
}
\end{frame}

\begin{frame}{Weights Calibration II}
\begin{itemize}
\item To calculate the weights the $\mathbf{x}_k$'s are only needed for the elements in the net sample (i.e. typically only for the respondents), but $\boldsymbol{\tau}_x$, their population totals need to be know.

\item The auxiliary variables can be metric (e.g. income or age) or categorical (e.g. gender or age groups).

\item Depending on the choice of $G_k$ different calibration estimators can be obtained, some of the most common are: \begin{itemize}
\item Post-stratification Estimator
\item Raking Estimator
\item Generalized Regression Estimator
\end{itemize}
 
\item Note that the $w_k$'s typically depend on the sample $\mathfrc{s}$, in contrast to the $d_k$, which are given by the sampling design.

\end{itemize}
\end{frame}

\begin{frame}{Post-stratification}

Post-stratification is typically used if only categorical auxiliary variables are available. It is implemented by forming weighting cells by crossing \emph{all} categories of the auxiliary variables.
These weighting cells are the post-strata $\mathcal{U}_q$ with $q=1,\ldots,Q$.
The weight are then adjusted to replicate the counts in these cells. For $k \in \mathcal{U}_q$  we have
$$g_k = \dfrac{\tau_{x_q}}{\hat{\tau}_{x_q}}\;{,}$$
 where $\tau_{x_q}= \sum_{k \in \mathcal{U}} x_{kq}$ and 
 $$
x_{kq}=
 \begin{cases}
 1 & \text{if}\; k \in \mathcal{U}_q \\
 0 & \text{else}
 \end{cases}.
 $$
 $\hat{\tau}_{x_q\,\pi}=\sum_{k \in \mathfrc{s}} d_k x_{kq}$ its estimator for $\tau_{x_q}$ based on the design weights. The auxiliary variables are the post-stratum indicators, i.e. $\mathbf{x}_k =(x_{k1}{,}\, x_{k2}{,}\,\ldots{,}\,x_{kQ})^\top$.
 An adjustment to the totals of a metric variable within the post-strata would also be possible.


\end{frame}


\begin{frame}[fragile]{Post-stratification}
<<ExPS,echo=FALSE, results='asis',cache=TRUE>>=

#Build a data set from HairEyeColor.
HairEyeColor. <- as.data.frame(HairEyeColor)
HairEyeColor. <- ddply(HairEyeColor., c("Hair","Eye"), .fun = function(x)c(Freq=sum(x$Freq)))

HairEyeColor.$ps <- apply(HairEyeColor.[,1:2],1,paste0,collapse=".")

HairEyeColor.inf <- with(HairEyeColor.,
                      merge(  data.frame(ID=1:sum(Freq),ps=rep(ps,times=Freq))
                             ,HairEyeColor.
                             ,by="ps"))


#For display
tau.tab <- xtable( apply(HairEyeColor,c(1,2),sum)
                  ,digits = 0, caption = "Population Counts $\\tau_{x_q}$ for Hair and Eye Colour")

align(tau.tab) <- "r|rrrr"

n <- 150
N <- nrow(HairEyeColor.inf)
HairEyeColor.inf$f1 <- N
HairEyeColor.inf$d <- N/n

# select  srswor of size n
set.seed(1345567)

sam <- sample(1:N, n)
samdat <- HairEyeColor.inf[sam, ]

# there must be at lest one persons in each poststratum
n.PS  <- table(Hair=samdat[, "Hair"], Eye=samdat[, "Eye"])
n.PS. <- as.data.frame(n.PS)
n.PS.$ps       <-  apply(n.PS.[,1:2],1,paste0,collapse=".")
n.PS.$Freq.hat <- n.PS.$Freq*N/n

n.PS.   <- merge(n.PS.,HairEyeColor.[,c("ps","Freq")],by="ps")
n.PS.$g <- n.PS.$Freq.y/n.PS.$Freq.hat
n.PS.$d <- N/n
n.PS.$w <- n.PS.$d*n.PS.$g

#n.PS.$w*n.PS.$Freq.x

#For display
tau.s   <- (n.PS)
tau.hat <- tau.s * N/n
g <- apply(HairEyeColor,c(1,2),sum)/(n.PS*N/n)

tau.s.tab <- xtable( tau.s
                     ,digits = 0
                     ,caption = paste0("Sample counts $\\sum_{k \\in \\mathfrc{s}} x_{kq}$ in a SRS with $n=",n,"$"))
tau.hat.tab <- xtable( tau.hat
                       ,digits = 4
                       ,caption = "Estimated totals $\\hat{\\tau}_{x_q\\,\\pi}=\\sum_{k \\in \\mathfrc{s}} x_{kq} d_k$ ")
g.tab <- xtable( g
                ,digits = 4
                ,caption = "Post-stratification $g_k = \\dfrac{\\tau_{x_q}}{\\hat{\\tau}_{x_q}}$ ")


align(tau.s.tab)   <- "r|rrrr"
align(tau.hat.tab) <- "r|rrrr"
align(g.tab)       <- "r|rrrr"

# print.xtable(tau.tab,hline.after = 0, caption.placement="top", sanitize.text.function = identity)
# print.xtable(tau.s.tab,hline.after = 0, caption.placement="top", sanitize.text.function = identity)
# print.xtable(tau.hat.tab,hline.after = 0, caption.placement="top", sanitize.text.function = identity)
# print.xtable(g.tab,hline.after = 0, caption.placement="top", sanitize.text.function = identity)


@
\onslide<1->{
<<ExPS.tauTab, echo=FALSE, results='asis'>>=
print.xtable(tau.tab,hline.after = 0, caption.placement="top", sanitize.text.function = identity)
@
}
\onslide*<2>{
<<ExPS.tauS, echo=FALSE, results='asis'>>=
print.xtable(tau.s.tab,hline.after = 0, caption.placement="top", sanitize.text.function = identity)
@
}
\onslide*<3>{
<<ExPS.tauHat, echo=FALSE, results='asis'>>=
print.xtable(tau.hat.tab,hline.after = 0, caption.placement="top", sanitize.text.function = identity)
@
}
\onslide<4>{
<<ExPS.g, echo=FALSE, results='asis'>>=
print.xtable(g.tab,hline.after = 0, caption.placement="top", sanitize.text.function = identity)
@
Beware, there must be at least one element in the sample from each post-stratum, otherwise we divide by null!
}
\end{frame}


\begin{frame}[fragile]{Raking}
\onslide*<1>{
In raking only the marginal totals are need, \emph{not} the totals for all the cross-categories.
Raking can be implemented as iterative post-stratification to adjust the design weights to the margins of the different auxiliary variables.
}
<<DataRaking,echo=FALSE,results='hide',message=FALSE>>=
# some setup
data(api)
MarginTab  <- table(apipop[,c("stype","sch.wide")])
MarginTab. <- cbind(MarginTab, SUM=rowSums(MarginTab))
MarginTab. <- rbind(MarginTab.,SUM=colSums(MarginTab.))
xtab <- xtable(MarginTab.,digits = 0
              ,caption = "Population Counts $\\tau_{x_q}$ for School Type (\\texttt{stype}) and School Target (\\texttt{sch.wide}) ")
align(xtab)<- "l|rr|r"
@

\begin{lrbox}{\mysavebox}
<<MarginTableRaking1, echo=FALSE>>=
#Th the 'apipop' data set from the 'survey' package
head(apipop[,c("dname","name","stype","sch.wide")])
@
\end{lrbox}
\onslide*<2>{
The design weights of a SRSC cluster sample of school districts are raked to 
variables school type (\texttt{stype}) and the accomplishment of the growth target (\texttt{sch.wide}).
\begin{center}
\usebox{\mysavebox}
<<MarginTableRaking2, echo=FALSE,results='asis'>>=
 print.xtable(xtab,hline.after = c(0,3), caption.placement="top", sanitize.text.function = identity)
@
\end{center}
}

\begin{lrbox}{\mysavebox}
<<AlogRakingPre, echo=TRUE, message=FALSE>>=
data(api)
set.seed(-57844)
#selection the SRCS
apiclus <- apipop[apipop$dnum%in%sample(unique(apipop$dnum),10),]
apiclus$fpc <- length(unique(apipop$dnum))

dclus1<- svydesign(id=~dnum, data=apiclus, fpc=~fpc)
#initial weight
w1    <- weights(dclus1)
#convergence is declared if the maximum change in a 
#table entry is less than 'eps' ...
eps   <- 1
#... otherwise the process stops after 'maxit' iterations
maxit <- 100

tau_stype    <- table(apipop$stype)
tau_sch.wide <- table(apipop$sch.wide)

#Raking (i.e. iterative post-stratification) for two variables
tab_x <- tab_y <- list()
@
\end{lrbox}
\onslide*<3>{\usebox{\mysavebox}}
\begin{lrbox}{\mysavebox}
<<AlogRakingLoop, echo=TRUE, message=FALSE,tidy=TRUE,size='tiny'>>=
for(i in 1:maxit){
##Post-stratification to the first variable
w1         <- split(w1,apiclus$stype)
adj1       <- tau_stype/sapply(w1, sum)
#new weight
w1.        <- w1 <- mapply(function(x,y)x*y,w1,adj1)
#return to original order
w1         <- unlist(w1.)
names(w1)  <- unlist(sapply(w1.,names))
w1         <- w1[as.character(sort(as.numeric(names(w1))))]
tab_x[[i]] <- tapply(w1,list(apiclus$stype,apiclus$sch.wide),sum)

##Post-stratification to the second variable
w2         <- split(w1,apiclus$sch.wide)
adj2       <- tau_sch.wide/sapply(w2, sum)
#new weight
w2.        <- w2 <- mapply(function(x,y)x*y,w2,adj2)
#return to original order
w2         <- unlist(w2.)
names(w2)  <- unlist(sapply(w2.,names))
w2         <- w2[as.character(sort(as.numeric(names(w2))))]
tab_y[[i]] <- tapply(w2,list(apiclus$stype,apiclus$sch.wide),sum)

if(i>1){
tab.diff <- abs(tab_y[[i-1]]-tab_y[[i]])

if(max(tab.diff)<eps)
 break
}
w1 <- w2

}
@
\end{lrbox}
\onslide*<4>{\usebox{\mysavebox}}
\end{frame}

%' \onslide*<5>{
%' <<AlogRakingCheck, echo=TRUE, cache=TRUE, message=FALSE,tidy=TRUE,size='scriptsize'>>=
%' #compare weights with the routine from the "survey" package
%' rdclust1 <- 
%'  rake(dclus1, list(~stype,~sch.wide)
%'                    ,list(table(stype=apipop$stype), table(sch.wide=apipop$sch.wide)))
%' weights(rdclust1)/w2
%' 
%' @
\begin{frame}[fragile]{Raking}
<<TabRakingIter,echo=FALSE,results='hide'>>=
n_I      <- length(unique(dclus1$cluster[[1]]))
sam.tab  <- svytable(~stype+sch.wide,dclus1)
sam.tab. <- cbind(sam.tab, SUM=rowSums(sam.tab))
sam.tab. <- rbind(sam.tab.,SUM=colSums(sam.tab.))

xtab     <- xtable(sam.tab.,digits = 1
                   ,caption = paste0("Estimated Totals $\\hat{\\tau}_{x_q\\,\\pi}=\\sum_{k \\in \\mathfrc{s}} x_{kq} d_k$ from a SRCS of Districts (\\texttt{dname})  with $n_{\\RN{1}}=",n_I,"$"))
align(xtab) <- "l|rr|r"
#print.xtable(xtab,hline.after = c(0,3), caption.placement="top", sanitize.text.function = identity)
#tab_x.tex <- tab_y.tex <- vector("list",i)
for(j in 1:i){

it.r.tab. <- cbind(tab_x[[j]], SUM=rowSums(tab_x[[j]]))
it.r.tab. <- rbind(it.r.tab.,SUM=colSums(it.r.tab.))  
tab_x.tex. <- xtable(it.r.tab.,digits = 1
                   ,caption = paste0("Estimated Totals after Adjustment to 'stype' in the ",j," Interation"))
align(tab_x.tex.)<- "l|rr|r"
print.xtable( tab_x.tex.,hline.after = c(0,3)
            ,caption.placement="top", sanitize.text.function = identity
            ,file=paste0("tables/rake-row-it",j,".tex"))
#opts_current$get("label")

it.c.tab. <- cbind(tab_y[[j]], SUM=rowSums(tab_y[[j]]))
it.c.tab. <- rbind(it.c.tab.,SUM=colSums(it.c.tab.))  

tab_y.tex. <- xtable(it.c.tab.,digits = 1
                   ,caption = paste0("Estimated Totals after Adjustment to 'sch.wide' in the ",j," Interation"))

align(tab_y.tex.)<- "l|rr|r"
print.xtable( tab_y.tex.,hline.after = c(0,3)
            ,caption.placement="top", sanitize.text.function = identity
            ,file=paste0("tables/rake-col-it",j,".tex"))

}
#probably better to write a hook for this...
path_Overlays_RakeTabs <- file(paste0(getwd(),"/input_Overlays_RakeTabs.tex") )
writeLines(paste0("\\only<",1:(i*2)+1,">{\\input{tables/",paste0(c("rake-row-it","rake-col-it")
                  ,rep(1:i,each=2)),".tex}}")
           ,con=path_Overlays_RakeTabs )
close(path_Overlays_RakeTabs )

@
\only<1>{
<<TabRakingIt0,echo=FALSE,results='asis'>>=
print.xtable(xtab,hline.after = c(0,3), caption.placement="top", sanitize.text.function = identity)
@
}
\input{input_Overlays_RakeTabs.tex}
\end{frame}


\begin{frame}[fragile]{Raking with the \texttt{survey} Package}
<<RakingWithSurvey>>=
dclus1r <- rake( dclus1, list(~stype, ~sch.wide)
                ,list( table(stype=apipop$stype) 
                      ,table(sch.wide=apipop$sch.wide)  
                ))
svytable(~stype+sch.wide, dclus1r , round=TRUE)

(w1/weights(dclus1r))[1:10]

summary(w1/weights(dclus1r))

@
\end{frame}

\begin{frame}{Generalized Regression Estimator}
For the linear generalized regression estimator (GREG) the measure of distance $G_k$ is
$$G_k(w,\pi,c)=G(w_k,d_k,c_k)=\dfrac{(w_k - d_k)^2}{2 d_k c_k} \;,$$
and we have
\begin{equation*}
\hat{\tau}_\text{GREG}= \hat{\tau}_\pi + \left( \boldsymbol{\tau}_{x}-
\hat{\boldsymbol{\tau}}_{x\,\pi}\right)^\top \widehat{\boldsymbol\beta},
\end{equation*}
where
\begin{equation*}
\widehat{\boldsymbol\beta}=\left( \underset{k \in \mathfrc{s} }{\sum }d_{k} c_k \mathbf{x}_{k} (\mathbf{x}_{k})^\top \right)^{-1}\underset{k \in \mathfrc{s}}{\sum } d_{k} c_k \mathbf{x}_{k} y_{k}\;{,}
\end{equation*}
and $\hat{\boldsymbol{\tau}}_{x\,\pi}=(\hat{\tau}_{x_1\,\pi}\,,\ldots\,,\hat{\tau}_{x_Q\,\pi} )^\top $.

The  adjustment to the design weight $g_k$ can be written as:
\begin{gather*}
g_k = 1 + \left(\Big(\sum_{k\in\mathcal{U}} \mathbf{x}_k - \sum_{k\in \mathfrc{s}} d_k \mathbf{x}_k\Big)^\top \Big(\sum_{k\in\mathfrc{s}} d_k c_k \mathbf{x}_k (\mathbf{x}_k)^\top \Big)^{-1}\right)^\top c_k \mathbf{x}_k
\end{gather*}
\end{frame}




% \pgfdeclareimage[height=7cm]{G1}{graphs/G1}
% \pgfdeclareimage[height=7cm]{G2}{graphs/G2}
% \pgfdeclareimage[height=7cm]{G3}{graphs/G3}

\begin{frame}{Graphical presentation of $\pi$ and GREG Estimator}
 ~\\[-1.5cm]
<<AuxUsage,echo=FALSE,cache=TRUE,out.height='8cm',out.width='\\linewidth', fig.pos='th', fig.show = 'animate',warning=FALSE, fig.keep='all',results='hide'>>=
set.seed(122335)
sigma <- matrix(c(10,4,4,3), ncol=2)
X <- rmvnorm(n=500, mean=c(1,2), sigma=sigma)
X <- X + abs(min(X))

plot(y=X[,1],x=X[,2],xlab="x",ylab="y")

s  <- sample(nrow(X),24)
Xs <- X[s,]

y.x.bar <- apply(Xs,2,mean)
x.mu  <- mean(X[,2])
lin.reg <- lm(Xs[,1]~Xs[,2])
abline(lin.reg,col=2,lwd=2)
points(x = y.x.bar[2],y=y.x.bar[1],pch=4,cex=3,col=4,lwd=2)

lines(x=c(min(X[,1])-1,y.x.bar[2]) , y =c(y.x.bar[1],y.x.bar[1]),lwd=2,col=4 )
text(labels=expression(bar(y)[pi])
      ,x = min(X[,2])+(y.x.bar[2]-min(X[,2]))/2, y=y.x.bar[1]
      ,pos = 3,col=4, lwd=2,cex=2)#, family="script")

lines(y=c(min(X[,1])-1,y.x.bar[1]) , x =c(y.x.bar[2],y.x.bar[2]),lwd=2,col=4 )
text(labels=expression(bar(x)[pi])
      ,y = min(X[,1])+(y.x.bar[1]-min(X[,1]))/2, x=y.x.bar[2]
      ,pos = 4,col=4, lwd=2,cex=2)#,family="script")


y.hat <- coef(lin.reg)[1] + coef(lin.reg)[2]*x.mu
lines(y=c(min(X[,1])-1, y.hat) , x =c(x.mu,x.mu),lwd=2,col=3 )
text(labels=expression(mu[x])
      ,y = min(X[,1])+(y.hat-min(X[,1]))/2, x=x.mu
      ,pos = 2, col=3, lwd=2,cex=2)#,family="script")

lines(x=c(min(X[,1])-1, x.mu)  , y =c(y.hat,y.hat),lwd=2, col=3)
text(labels=expression( bar(y)[w] )
      ,x = min(X[,2])+(x.mu-min(X[,2]))/2, y=y.hat
      ,pos = 1, col=3, lwd=2,cex=2)#,family="script")





@
\end{frame}

\begin{frame}[fragile]{Generalized Regression Estimator}
\begin{lrbox}{\mysavebox}
<<GREG.data,warning=FALSE,message=FALSE,cache=TRUE>>=
library(PracTools) #load the package
data(smho.N874)    #load the data set
head(smho.N874)
#?smho.N874         #for a description of the variables

#only hospitals other than 'type 4' are considered
smho <- smho.N874[smho.N874$hosp.type != 4, ]
@
\end{lrbox}
\onslide*<1>{
We want to estimate total expenditures of hospitals. To improve a possible estimate we use data from survey in 1998 to explore if there are any useful predictors for our variable of interest.
\usebox{\mysavebox}}
\onslide*<2>{
~\\[-0.75cm]
<<GREG.PlotMa,message=FALSE,echo=FALSE,cache=TRUE,dependson='GREG.data',out.height='8cm', out.width='\\linewidth', fig.pos='ht', warning=FALSE, results='hide'>>=
pairs.chrt <- 
  ggpairs(smho,columns = which(colnames(smho)%in%c("EXPTOTAL","BEDS","SEENCNT","EOYCNT"))) +
    theme(legend.position = "none",
        panel.grid.major = element_blank(),
        axis.ticks = element_blank(),
        axis.title.x = element_text(angle = 90, vjust = 1, color = "black"),
        panel.border = element_rect(fill = NA),
        axis.text.x =element_text(size=8,angle = 90) ) 

print(pairs.chrt, left = 0.5, bottom = 0.35)

# panel.cor <- function(x, y, digits = 2, cex.cor, ...)
# {
#   usr <- par("usr"); on.exit(par(usr))
#   par(usr = c(0, 1, 0, 1))
#   # correlation coefficient
#   r <- cor(x, y)
#   txt <- format(c(r, 0.123456789), digits = digits)[1]
#   txt <- paste("r= ", txt, sep = "")
#   text(0.5, 0.6, txt)
# 
#   # p-value calculation
#   p <- cor.test(x, y)$p.value
#   txt2 <- format(c(p, 0.123456789), digits = digits)[1]
#   txt2 <- paste("p= ", txt2, sep = "")
#   if(p<0.01) txt2 <- paste("p= ", "<0.01", sep = "")
#   text(0.5, 0.4, txt2)
# }
# 
# pairs(smho[,colnames(smho)%in%c("EXPTOTAL","BEDS","SEENCNT","EOYCNT")], upper.panel = panel.cor)
@
' }
\onslide*<3>{Fitting a linear model for \texttt{EXPTOTAL} with common slopes for \texttt{SEENCNT} and \texttt{EOYCNT} but a different slope for \texttt{BEDS} in each hospital type.
<<GREG.model,warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE, dependson='GREG.data',results='asis'>>=
smho. <- smho
smho.$hosp.type <- as.factor(smho.$hosp.type)
smho.$FINDIRCT  <- as.factor(smho.$FINDIRCT)
lmod1 <- lm(EXPTOTAL ~ SEENCNT + EOYCNT + FINDIRCT + hosp.type:BEDS, data=smho.)
tab.model <- xtable(summary(lmod1),digits = 2,caption = "Model Summary")
print(tab.model,caption.placement = "top")
@
}\onslide*<4>{
~\\[-1.25cm]
<<GREG.PlotModelDiag,warningFALSE, message=FALSE, echo=FALSE, cache=TRUE, dependson='GREG.data',results='asis',fig.keep='all',out.width='\\linewidth',out.height='8cm',fig.pos='ht',fig.show = 'animate'>>=
par(mfrow=c(1,1))
plot(lmod1,ask=FALSE)
@
}
\end{frame}

\begin{frame}[fragile]{Generalized Regression Estimator}
\begin{lrbox}{\mysavebox}
<<GREG.sample,warning=FALSE, message=FALSE, cache=TRUE, dependson='GREG.data'>>=
#######################################
### Select a pps to sqrt(BEDS) sample
#######################################
library(sampling)   #load the 'sample' package 
                    #for the 'UPsystematic' function
smho. <-   # before sampling order the data set by hospital type
  smho.[order(smho.$hosp.type),] 

x <- smho.[,"BEDS"]
x[x <= 5] <- 5      # recode small hospitals to have a minimum size
x <- sqrt(x)

n <- 80             #sample size
IP  <- n*x/sum(x)

set.seed(428274453)
sam <- UPsystematic(IP)

sam.dat <- smho.[sam==1, ]
sam.dat$d <- 1/IP[sam==1] #the design weight
@
\end{lrbox}
\onslide*<1>{~\\[-1cm] We select a sample of hospitals with probability  proportional to the square root of \texttt{BEDS} using a systematic sample. 
 \usebox{\mysavebox}
}
\begin{lrbox}{\mysavebox}
<<GREG.calib, message=FALSE, cache=TRUE, dependson='GREG.sample'>>=
library(survey) #load the 'survey' package 
#1. build a 'design' object
sam.dsgn <- 
  svydesign(ids = ~1,               # no clusters
            strata = NULL,          # no strata
            data = sam.dat,         # the sample data 
            weights = ~d)           # the design weight
    #the model we use for the GREG
lmod2 <- lm(EXPTOTAL ~ SEENCNT + EOYCNT + hosp.type:BEDS, data=smho.)
#2. compute pop totals of auxiliaries
pop.tots <- colSums(model.matrix(lmod2)) #Inefficient but convenient!

#3. use 'calibrate' to compute the new weights
sam.cal <- 
  calibrate(design = sam.dsgn,
            formula = ~ SEENCNT + EOYCNT + hosp.type:BEDS,
            population = pop.tots,
            calfun='linear' )
@
\end{lrbox}
\onslide*<2>{ Now we use the \texttt{survey} package to calibrate the weights.
 \usebox{\mysavebox}
Setting argument \texttt{calfun='linear'} in \texttt{'calibrate'} results in the GREG weights, other calibration function are possible, already built-in are \texttt{'raking'} and \texttt{'logit'}. 
}
\begin{lrbox}{\mysavebox}
<<GREG.checkW, message=FALSE, cache=TRUE, dependson='GREG.calib'>>=
#BEDS by hospital type
svyby(~BEDS, by=~hosp.type, design=sam.cal, FUN=svytotal)
#SEENCNT and EOYCNT
svytotal(~SEENCNT+EOYCNT, sam.cal)
pop.tots 
@
\end{lrbox}
\onslide*<3>{~\\[-0.9cm]
Now we check if the calibration constrains are satisfied:
\usebox{\mysavebox}
}

\end{frame}

\begin{frame}{Generalized Regression Estimator}
\begin{itemize}
  \item<1> Nothing prevents the GREG weights from becoming negative, which is theoretically not a problem, as long as we infer to the population (or sub-populations) to which we calibrated. 
  \item<2> However the effects might be catastrophic of domain estimation, in case of estimation domains that where not considered in the calibration. 
  \item<3> In general it is advisable to only use calibrated weights to infer to the whole population or sub-populations that are found in the marginal totals used for the calibration!
  \item<4> Design weights can always be used to do unbiased domain estimation, although the precision of these estimates can be very poor.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Generalized Regression Estimator}
It is possible to add some additional constraints to the calibration problem to ensure that the resulting weights do not deviate to much from the input weights (e.g. the design weights), thus reducing the risk of having negative weights. 
<<GREG.wBD, message=FALSE, cache=TRUE, dependson='GREG.calib',warning=FALSE>>=
#GREG with bounds
sam.calBD  <- 
  calibrate(design = sam.dsgn,
            formula = ~ SEENCNT + EOYCNT + hosp.type:BEDS,
            population = pop.tots,
            bounds = c(0.5,2),   
            calfun='linear' )
#ratio without bounds and with them
rbind(noBD=summary(weights(sam.cal)/weights(sam.dsgn)),
      BD=summary(weights(sam.calBD)/weights(sam.dsgn)))
@
The bounds are relative, i.e the values of the bound argument are the upper and lower limit of $\frac{w_k}{d_k}$, for all $k \in \mathfrc{s}$. 
Note that if the bounds are too tight the calibration might fail.
\end{frame}


% \begin{frame}
% \frametitle{Two-phase Approach}
% 
% 
% \end{frame}
% 

\begin{frame}[fragile]{The Tow-Phase Approach to \\ Non-response Weighting}
\begin{lrbox}{\mysavebox}
<<2P.data,warning=FALSE,message=FALSE>>=
library(PracTools) #load the package
data(nhis)         #load the data set
head(nhis)
@
\end{lrbox}
\onslide*<1>{
We use to 2003 NHIS data set from the \texttt{PracTool} package to fit a \emph{generalized linear models} (GLM) which we will use to predict the RP's.
\usebox{\mysavebox}
}
\begin{lrbox}{\mysavebox}
<<2P.modelFit,echo=1:10>>=
#some editing
nhis. <- nhis
nhis.$hisp      <- as.factor(nhis.$hisp)
nhis.$parents_r <- as.factor(nhis.$parents_r)
nhis.$educ_r    <- as.factor(nhis.$educ_r)

#fitting a model of binomial data using the 'logit' link function
glm.logit <- glm(resp ~ age + hisp +  
                 parents_r + educ_r,
                 family=binomial(link = "logit"), 
                 data = nhis.)



glm.logit.sum <- summary(glm.logit)
tab.glm.model <- xtable(glm.logit.sum ,digits = 2, caption = "Model Summary")
@
\end{lrbox}
\onslide*<2>{
The variable \texttt{resp} is the respondent indicator (0 = non-respondent; 1 = respondent) the other variables in the data set are either socio-demographic variables or metadata on the sampling design, i.e. information that was available regardless of the responds behavior. 
\usebox{\mysavebox}
}
\onslide*<3>{
<<2P.modelSum,echo=FALSE,results='asis'>>=
print(tab.glm.model,caption.placement = "top")
@
}
\begin{lrbox}{\mysavebox}
<<2P.newWeightse>>=
psi.logit <-
 predict(glm.logit, type ='response')

nhis.$new.svywt <- (1/psi.logit)*nhis.$svywt

#the mean response rate for the MAR and MCAR model are the same
mean(psi.logit);mean(nhis.$resp)

#comparing MAR and MCAR by education 
rbind(MAR=by(nhis.,  nhis.$educ_r, 
             function(x)  sum(x$new.svywt) ),
      MCAR=by(nhis., nhis.$educ_r, 
              function(x)  sum( x$svywt* 1/mean(nhis.$resp) ) )
)

@
\end{lrbox}
\onslide*<4>{
Now we compute the tow-phase weights:
\usebox{\mysavebox}
}
\begin{lrbox}{\mysavebox}
<<2P.WGLM,echo=1:4>>=
#create the survey design object
nhis.dsgn <- svydesign(ids = ~psu,
          strata = ~stratum,
          data = nhis.,
          nest = TRUE,
          weights = ~svywt)

wglm.logit <- 
   svyglm(glm.logit$formula,
          family=binomial(link = "logit"),
          design = nhis.dsgn)


wglm.logit.sum <- summary(wglm.logit)
tab.wglm.model <- xtable(wglm.logit.sum ,digits = 2, caption = "Model Summary")
@
\end{lrbox}
\onslide*<5>{
As an alternative the GLM model can also be fitted with design weights using 
the \texttt{svyglm} function from the \texttt{survey} package.
\usebox{\mysavebox}
}
\onslide*<6>{
<<2P.bothModelSum,echo=FALSE,results='asis'>>=
both.glm <- cbind(tab.wglm.model,tab.glm.model)
both.glm <- both.glm[,-grep(colnames(both.glm),pattern = "value")]
tab.both.glm<- xtable( both.glm,digits=2
                      ,caption="Weighted and Unweighted Parameter Estimates from Logistic Models")


addtorow <- list()
addtorow$pos <- list(-1)
addtorow$command <- paste0(' \\hline & \\multicolumn{3}{c|}{Survey Weighted} & \\multicolumn{3}{c}{Unweighted}\\\\')


print(tab.both.glm,caption.placement = "top",add.to.row = addtorow
      ,size = 'footnotesize') 

@
Beware, \texttt{glm} has also a weight argument, but its in general a bad idea to supply the survey weights directly to it!
}
\end{frame}

\begin{frame}[allowframebreaks]\frametitle{Literature}    
%\scriptsize
  \begin{thebibliography}{10}    
   \setbeamertemplate{bibliography item}[book]
  \bibitem{LittleRubin2002}
  R.J.A.~Little, D.B.~Rubin.
    \newblock  Statistical Analysis with Missing Data.
    \newblock {\em Wiley Interscience}, 1999.
  \setbeamertemplate{bibliography item}[article]
  \bibitem{Rubin1976}
    D.B.~Rubin.
  \newblock  Inference and Missing Data
  \newblock {\em Biometrika}, 1976.
  \setbeamertemplate{bibliography item}[book]
  \bibitem{Rubin1987}
    D.B.~Rubin.
  \newblock  Multiple Imputations for Nonresponse in Surveys
  \newblock {\em Wiley}, 1987.  
  \end{thebibliography}
\end{frame} 


% \begin{frame}{Two-phase Approach with a GLM}
%  
%  
% \end{frame}
 
 %System time of variance calculation
 
 %Variance of the GREG
 
 %Taylor linearization of a ratio estimatir V(x/y)\neq V(x)/V(y) not even E(x/y) \neq E(x)/E(y) is close enought!
 %Influence Funktions
 
 %Resampling Methods

%Ratio estimator  Saerndal2005 42

\end{document}
