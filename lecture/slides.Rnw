% !Mode:: "TeX:DE:UTF-8:Main"

\documentclass{beamer}
\usepackage{tikz}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lipsum}

\usepackage{textpos} 
\usepackage{hyperref}
\usepackage{amsmath,amsthm,amsfonts,nicefrac,mathabx,amssymb}
\usepackage[subnum]{cases}
\usepackage{calligra, mathrsfs}
%\usepackage{natbib}
\usepackage{booktabs}
%\bibpunct{(}{)}{;}{a}{,}{,}

\usepackage{helvet}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow,dcolumn}
\usepackage{ragged2e}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{url}
\usepackage{bibentry}
\usepackage{chngcntr}

\usepackage{animate} %for animation

\usetheme{Gesis}


\newcommand{\eqname}[1]{\tag*{#1}} %equation title

%new math cal fonds
\newenvironment{frcseries}{\fontfamily{frc}\selectfont}{}
\newcommand{\textfrc}[1]{{\frcseries#1}}
\newcommand{\mathfrc}[1]{\text{\textfrc{#1}}}

\setcounter{tocdepth}{1}
\setbeamercolor*{section in toc}{fg=hellgrau}
\setbeamertemplate{bibliography item}[default]


\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}


\newcommand{\E}[1]{\text{E}\left(#1\right)}
\newcommand{\V}[1]{\text{V}\left(#1\right)}
\newcommand{\Vest}[1]{\widehat{\text{V}}\left(#1\right)}
\newcommand{\MSE}[1]{\text{MSE}\left(#1\right)}
\newcommand{\COV}[2]{\text{COV}\left(#1,\,#2\right)}

\newcommand{\RN}[1]{%
  \textup{\uppercase\expandafter{\romannumeral#1}}%
}


\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{\texorpdfstring%
  {L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
  {LyX}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
% this default might be overridden by plain title style
 \newcommand\makebeamertitle{\frame{\maketitle}}%
 % (ERT) argument for the TOC
 \AtBeginDocument{%
   \let\origtableofcontents=\tableofcontents
   \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
   \def\gobbletableofcontents#1{\origtableofcontents}
 }

% A Section Title Slides
\AtBeginSection{\frame{\sectionpage}}
\AtBeginSubsection{\frame{\subsectionpage}}


\title{Sample Theory}
\subtitle{Epidemiological Study Design and Statistical Methods}

\author{Stefan Zins - GESIS}
\date{12.12.2017}

% Gestaltungsvarianten
% Kopfbandfarbe "shaded" von weiß zu blau: 
% \setbeamertemplate{gesisheadband}[shaded]
% andere Farbwerte
% \gesisCIbluecolors % = Blau und Orange aus CI, Blau als heller Hintergrund
% \gesisCIcolors     % = Blau und Orange aus CI, Sand als heller Hintergrund



%
% knitr setup 
% ------------------------------------------------------------------------------   
<<setup, echo=FALSE, results='hide', warning=FALSE, message=FALSE>>=
#### Load and cite R packages ####
# Create list of packages
PackagesUsed <- c("knitr", "xtable", "repmis",
                  "sampling","survey","mvtnorm",
                  "magrittr","dplyr","reshape2",
                  "tables","ggplot2","PracTools"
                  )

# Load PackagesUsed and create .bib BibTeX file
# Note must have repmis package installed.
repmis::LoadandCite(PackagesUsed, file = "Packages.bib", install = FALSE)


#### Set chunk options ####
opts_chunk$set(fig.align='center')

#hook for inline scientific notation
knitr::knit_hooks$set(inline = function(x) {
  knitr:::format_sci(x, 'md')
})

@




\begin{document}

\gesismaketitle % erzeugt Titelseite

\begin{frame}%[t]
  \frametitle{Content}
  
  \begin{itemize}
  \item Design Based Inference
  \item Sampling Designs
  \item Sample Size Planning
  \end{itemize}


\emph{SamplignAndEstimation} on GitHub, branch sampling\_short:
\url{https://github.com/BernStZi/SamplingAndEstimation/tree/sampling_short}
\end{frame}


\begin{frame}{Motivation}
 \begin{itemize}
    \item<1>  Did you ever work with sample data? 
    \item<2> What kind of analysis have you done with sample data?
    \item<3>  What concerns did you have while applying your analytically methods?
  \end{itemize}
\end{frame}



\section{Design Based Inference}



\begin{frame}{\alert{Finite} Population, Sample, and Sampling Design}

 \begin{itemize}
 \item[] 
 \begin{align}
 \mathcal{Y} = & \{ y_1{,}y_2{,}\,\ldots{,}\,y_k{,}\,\ldots{,}\,y_N \} \eqname{finite population of size $N$} \\
 \mathcal{U} = &\, \{ 1{,}2{,}\,\ldots{,}\,k{,}\,\ldots{,}\,N \} \eqname{sampling frame} \\
 \mathfrc{s} \subset &\, \mathcal{U} \eqname{sample of size $n$} \\
 \mathcal{P}(\mathcal{U}) & \eqname{all possible subsets of $\mathcal{U}$}
 \end{align}
 \item[] The discrete probability distribution $p(.)$ over $\mathcal{P}(\mathcal{U})$ is called a \emph{sampling design} and  $\mathcal{G}=\{ \mathfrc{s} | \mathfrc{s} \in \mathcal{P}(\mathcal{U}),\, p(\mathfrc{s}) > 0 \}$ is called the support of $p(.)$ with
$$
\sum_{\mathfrc{s} \in \mathcal{G}} p(\mathfrc{s}) = 1\;.
$$
 \end{itemize}
\end{frame}


\begin{frame}{Estimation}
\begin{align}
 \theta       & = f(\mathcal{Y})  \eqname{statistic of interest} \\
 \hat{\theta} & = f(\mathcal{Y}, \mathfrc{s} )  \eqname{estimator for $\theta$} \\
 \E{\hat{\theta}} & = \sum_{\mathfrc{s} \in \mathcal{G}} p(\mathfrc{s}) f(\mathcal{Y}, \mathfrc{s} )   \eqname{expected value of $\hat{\theta}$} \\
 \V{\hat{\theta}}   & =  \E{\hat{\theta}^2} -  {\E{\hat{\theta}}}^2 \eqname{variance of  $\hat{\theta}$} 
\end{align}
 $\E{.}$ and $\V{.}$ are always with respect to the sampling design $p()$ and
 an estimator is said to be unbiased if
 $$ \E{\hat{\theta}} = \theta\;. $$
\end{frame}

\begin{frame}{Law of Large Numbers (LLN)}
Weak law of large numbers: 

Suppose $ \{ y_1{,}y_2{,}\,\ldots{,}\,y_k{,}\,\ldots{,}\,y_N \}$ is a sequence of i.i.d. random variables with mean $\mu$ and $\mu \neq \infty$ and $\mu \neq -\infty$. Then for $n \rightarrow \infty$ then we have:

$$
\bar{y} \xrightarrow{P} \mu
$$

If the LLN holds we can have unbiased estimates, as our estimates will converge in probability to their expected (true) value. That is, it assures $\E{I_k}=\pi_k$.
\end{frame}


\begin{frame}{LLN Demonstration}
\onslide*<1>{
Suppose all $y_i$ follow an exponential distribution with mean and variance equal to one. We take repeatedly a sample of size $50$. For each sample the sample mean is calculated. The mean of the sample means should converge towards the true mean of the distribution with increasing number of samples.
}
\onslide<2>{
<<LLN, echo=FALSE, cache=TRUE, fig.height=5, out.width='.8\\linewidth', fig.pos='th',  warning=FALSE, results='hide'>>=
set.seed(1829)

#setting a parameters of Bi(n, p)
r <- 1000
n <- 50
p <- 0.4

#dataframe
df <- data.frame(y = rexp(10000, rate=1) )

OUT <- sapply(1:r, function(x) mean(rexp(n, rate=1)) )

#graph
plot(cumsum(OUT)/(1:r), type='l',
     main = "Simulation of the Low of Large Numbers",
     xlab="Number of Samples", ylab="Mean of Sample Means", 
     ylim=c(0.98,1.05))
abline(h = 1, col="red")
@
}
\end{frame}

\begin{frame}{Central  Limit Theorem (CLT)}
CLT of \emph{Lindeberg–L\'evy}: \\
Suppose $\{ y_1{,}y_2{,}\,\ldots{,}\,y_k{,}\,\ldots{,}\,y_N \}$ is a sequence of i.i.d. random variables with $V(y_i) < \infty\;\forall\; i=1,\,\ldots,\,N$. Then for $n \rightarrow \infty$ then we have:
$$
\dfrac{\bar{y}-\mu}{\sigma \sqrt {n}} \ {\xrightarrow {d}}\ N\left(0,1\right)
$$
If the CLT holds, symmetric confidence intervals can be constructed with quantiles from the standard normal distribution $\Phi(z)$
$$
\left[ \bar{y} + \Phi(\alpha/2)\sigma\sqrt{n} \,;\, \bar{y} + \Phi(1-\alpha/2)\sigma\sqrt{n}\right]
$$

%driven by sample size
\end{frame}


\begin{frame}{CLT Demonstration}
<<AuxUsage, echo=FALSE, cache=TRUE,warning=FALSE, results='hide'>>=
set.seed(122335)

sdm.sim <- function(n,src.dist=NULL,param1=NULL,param2=NULL,r) {
  #r <- 10000  # Number of replications/samples - DO NOT ADJUST
  # This produces a matrix of observations with  
  # n columns and r rows. Each row is one sample:
  my.samples <- switch(src.dist,
                       "E" = matrix(rexp(n*r,param1),r),
                       "N" = matrix(rnorm(n*r,param1,param2),r),
                       "U" = matrix(runif(n*r,param1,param2),r),
                       "P" = matrix(rpois(n*r,param1),r),
                       "B" = matrix(rbinom(n*r,param1,param2),r),
                       "G" = matrix(rgamma(n*r,param1,param2),r),
                       "X" = matrix(rchisq(n*r,param1),r),
                       "T" = matrix(rt(n*r,param1),r))
#  all.sample.sums <- apply(my.samples,1,sum)
  
  all.sample.means <- apply(my.samples,1,mean)   
  #all.sample.vars  <- apply(my.samples,1,var) 
  
  all.sample.means.sd <- (all.sample.means - param1^{-1})/sqrt(param1^{-2}/n)
  
  par(mfrow=c(1,2))
  #hist(my.samples[1,],col="gray",main="Distribution of One Sample")
  #hist(all.sample.sums,col="gray",main="Sampling Distribution\nof
	#the Sum")
  hist(all.sample.means.sd,col="gray",main=paste0("Sampling Distribution\nof Sample Means, n=",n),
       nclass=ceiling(sqrt(r)),
       probability = T,
       xlab="Sample Quantiles"
       )
  lines(density(all.sample.means.sd), lwd=2, col=4 )
  x <- seq(min(all.sample.means.sd ),max(all.sample.means.sd ),length=1000)
  y <- dnorm(x, mean=0, sd=1 )
  lines(x,y , type="l", lwd=2, col=2)  
  #-----
  qqnorm(all.sample.means.sd)
  qqline(all.sample.means.sd,col=2, lwd=2)
  pvalue.ksTest <- ks.test(all.sample.means.sd,"pnorm")$p.value
  text(x=-4,y=max(all.sample.means.sd)-0.5, 
       labels = paste0("p-value KS-Test: ",round(pvalue.ksTest,4)),
       pos = 4, cex = 0.8)
  
  #hist(all.sample.vars,col="gray",main="Sampling Distribution\nof
	#the Variance")
}
@


\onslide*<1>{
Suppose all $y_i$ follow an exponential distribution with mean and variance equal to one.
<<CLT_1, echo=FALSE, cache=TRUE, out.height='5cm', fig.pos='th', warning=FALSE, fig.keep='all', results='hide'>>=
  x <- seq(0, 10, length=1000)   
  y <- dexp(x, r=1)  
  plot(x,y , type="l", lwd=2, col=2,  main="Exponential Distribution", ylab="Density",xlab="Quantile")
@
}

\onslide*<2>{
<<CLT_2, echo=FALSE, cache=TRUE, out.height='6.75cm', warning=FALSE,  results='hide'>>=
sdm.sim(5,src.dist="E",param1=1,r=10000)       
@
 }

 \onslide*<3>{
<<CLT_3, echo=FALSE, cache=TRUE, out.height='6.75cm', warning=FALSE,  results='hide'>>=
 sdm.sim(50,src.dist="E",param1=1,r=10000)     
 @
 }

\onslide*<4>{
<<CLT_4, echo=FALSE, cache=TRUE, out.height='6.75cm', warning=FALSE,  results='hide'>>=
sdm.sim(500,src.dist="E",param1=1,r=10000)     
@
}
\onslide*<5>{
<<CLT_5, echo=FALSE, cache=TRUE, out.height='6.75cm', warning=FALSE,  results='hide'>>=
sdm.sim(5000,src.dist="E",param1=1,r=10000)     
@ 
}
\end{frame}

\begin{frame}{Inclusion Probabilities}
\begin{align}
 I_k   &=   \begin{cases}   1  & \text{if}\; k \in \mathfrc{s} \\
                            0  & \text{else}  
          \end{cases}   \eqname{sampling indicator element $k$} \\
\E{I_k}     &  =   \pi_k    \eqname{inclusion probability of element $k$ } \\
\E{I_k I_l} &  =   \pi_{kl}  \eqname{joint expectation of $I_k$ and $I_l$} \\
\sum_{k \in \mathcal{U}}\pi_k & = \E{n} \eqname{expected sample size}
\end{align}

The $I_k$ are the \emph{only} random variables in the design based frame work and they follow a theoretical distribution. E.g. a Hypergeometric distribution for SRS. 

\end{frame}

\begin{frame}{Inclusion Probabilities}
With the inclusion probabilities design unbiased estimators can be constructed. For example an estimator for a total $\tau=\sum_{k \in \mathcal{U}} y_k$.
\begin{align*}
 \hat\tau &= \sum_{k \in \mathfrc{s}} \dfrac{y_k}{\pi_k} & \E{\hat\tau } &= \sum_{k \in \mathcal{U}} E(I_k) \dfrac{y_k}{\pi_k} = \tau
\end{align*}
$\pi_k^{-1}$ is also called the \alert{design weight} of element $k$.
% Many estimator can be written as functions of totals, which makes it possible to have at least design consistent estimtors for them.

$V(\hat\theta)=f(\mathcal{Y},\Sigma)$, with $\Sigma=(\E{I_k I_l}-\E{I_k} \E{I_l})_{k,l=1,\ldots,N}$.
For complex sampling designs $\Sigma$ can be very complex too and difficult to compute. In practice it is thus often unknown to data users. However there are approximations to $V(\hat\theta)$ that only require the $\pi_k$'s and are much simpler to estimate than $V(\hat\theta)$.

\end{frame}


\begin{frame}{Sample Mean with SRS}
  \small
  \begin{align*} 
  \mu = \dfrac{1}{N} \sum_{k \in \mathcal{U}}  y_k{,}  && \overline{y} = \sum_{k \in \mathfrc{s}}  \dfrac{y_k}{n}{,} && \sigma^2 = \dfrac{1}{N}\sum_{k \in \mathcal{U}} (y_k - \mu )^2{,}
  && V^2 = \sigma^2 \dfrac{N}{N-1}
  \end{align*}
  ~\\[-0.5cm]
  \begin{columns}[t]
   \onslide<2->{
    \begin{column}{.5\textwidth}
      \begin{align*} 
        \E{\overline{y}} & = \E{\sum_{k \in \mathcal{U}} I_k \dfrac{y_k}{n} } \\
        & = \dfrac{1}{n} \sum_{k \in \mathcal{U}} \E{I_k} y_k  \\
        & = \dfrac{1}{n} \sum_{k \in \mathcal{U}} \pi_k y_k  \\
        & = \dfrac{1}{N} \sum_{k \in \mathcal{U}}  y_k  
       \end{align*}
      \end{column}
    \begin{column}{.5\textwidth}
   }
   \onslide<3>{
      \begin{align*} 
        \V{\overline{y}} & = \V{\sum_{k \in \mathcal{U}} I_k \dfrac{y_k}{n}} \\
        & = \dfrac{1}{n^2} \sum_{k \in \mathcal{U}} \sum_{l \in \mathcal{U}} \COV{I_k}{I_l} y_k y_l  \\
        & = -\dfrac{1}{2} \dfrac{1}{n^2} \sum_{k \in \mathcal{U}} \sum_{l \in \mathcal{U}} (\pi_{kl}-\pi_k\pi_l)  \left(y_k - y_l \right)^2  \\
        & = \dfrac{N-n}{N-1} \dfrac{\sigma^2}{n}  =   \left( 1 -  \dfrac{n}{N} \right) \dfrac{V^2}{n}
      \end{align*}
    \end{column}
   }
    \end{columns}
 \end{frame}



\begin{frame}{Model-based Approach}
The sample data: $\mathfrc{y}=\{y_1{,}\,\ldots{,}\,y_k{,}\,\ldots{,}\,y_n\}$.
All $y_k \in \mathfrc{y}$ are independent identical distributed (iid) random variables, with
\begin{equation*}
  y_k \sim NV(\mu, \sigma)\;{.}
\end{equation*}
~\\[-1cm]
\begin{columns}[t]
   \onslide<2->{
    \begin{column}{.5\textwidth}
      \begin{align*} 
        \E{\overline{y}}_{M} & = \E{\sum_{k \in \mathfrc{s}} \dfrac{y_k}{n} } \\
        & = \dfrac{1}{n} \sum_{k \in \mathfrc{s}} \mu  \\
        & = \mu
       \end{align*}
      \end{column}
    \begin{column}{.5\textwidth}
   }
   \onslide<3->{
      \begin{align*} 
        \V{\overline{y}}_{M} & = \V{\sum_{k \in \mathfrc{s}} \dfrac{y_k}{n}}_{M} \\
        & = \dfrac{1}{n^2} \sum_{k \in \mathfrc{s}} \sigma^2 \\
        & = \dfrac{\sigma^2}{n}    
      \end{align*}
    \end{column}
   }
    \end{columns}
     \onslide<4>{Note that there is no finite population correction.}
\end{frame}


\section{Sampling Designs}


\begin{frame}{Representative Sample}
\onslide*<1-2>{What is a representative sample? \newline}
\onslide*<2>{The popular concept of a representative sample it that the sample is a \emph{miniature} of the population.}

\onslide<3-4>{However, what do we actually want?}
\onslide<4>{\newline We want to estimate a statistic of interest with a certain level of precision 
 and if the level of precision is high enough we say our estimation \emph{strategy} is representative.}
\end{frame}

\begin{frame}{Sampling Frames}

Access to the target population is of major importance for the selection of any sample.
This is often done with the help of a sampling frame, a register that links observational units to a identifier units. Then the units of the register can be sampled. 

\onslide*<2>{
For samples of persons popular sampling frame are:
\begin{itemize}
\item Address Registers 
 \begin{itemize}
  \item Address of buildings
  \item Address of dwellings
  \item Address of persons
  \item Address for post delivery points
 \end{itemize}
\item Telephone number 
\begin{itemize}
  \item Set of possible landline numbers
  \item Set of possible mobile numbers
  \item Union of possible landline and mobile numbers (Multi-Frame)
 \end{itemize}
\end{itemize}
}
\onslide<3>{

 ~\\
 Ideally the sampling frame should have one and one entry only for each observational unit of the target population. In practice it is often difficult to find such a  \emph{perfect} sampling frame, i.e. without any over or under coverage. \\

And Some sampling designs do not use a sampling frame at all.


}
\end{frame}


\begin{frame}{Probability Based Samples}
Probability Samples - A finite set of possible samples each having a certain probability of being selected, given by the sampling design. The sampling design should be measurable, that is:
\begin{itemize}
\item $\pi_{k}>0 \; \forall\; k \in \mathcal{U}$
\item $\pi_{kl}>0 \; \forall\; k \neq l \in \mathcal{U}$
\end{itemize}

\onslide<2>{
Probability based samples do not require any (parametric) assumptions about the data to analyse them, in that respect they can be considered a robust strategies.
}

\end{frame}

\begin{frame}{Non-Probability Based Samples}
Non-Probability Samples - Literally speaking the sampling method should select always the same sample if repeated. Often used for selection processes that are less controlled and often to complex to be modeled. 
\onslide<2->{
Examples:
\begin{itemize}
 \item Convenience Samples
 \item Purposive   Samples
 \item Opt-in      Samples 
 \begin{itemize}
  \item (Online) Access Panels  
  \item Invitations to a survey on webpages
 \end{itemize}
 \item Quota       Samples
\end{itemize}
}
\onslide<3>{
Non-probability based samples require (often unverifiable) assumptions about the observed data to analyse it (model-based inference). They often lack a theoretical frame work that could be used to construct unbiased or consistent estimates. \\
}
\end{frame}


%alter

\begin{frame}{Sampling Algorithms}
A sampling algorithm is a set of rules used to select a sample from a population. Two distinctions can be made:
\begin{itemize}
\item<2-> Algorithms for simple random sampling. 
All samples have the same probability of being selected, i.e. $p(\mathfrc{s})$ is constant for all possible samples.
\item<3-> Algorithms for unequal probability sampling. There can be difficulties with non-random samples sizes, (other constraint such as balancing on auxiliary variables) and large sampling frames.
\end{itemize}

\onslide<4>{
A sequential sampling algorithm can be applied to a sampling frame. That is, it is not necessary to enumerate all samples in a support of sampling design to select one of them.
}
\end{frame}


\begin{frame}{Systematic Sampling}
The elements of the population are brought into a specific ordered and $V^i = \sum_{k=1}^i \pi_k$. A value $\lambda$ is selected from a uniform distribution between 0 and 1.
~\\[-3cm]
<<UP_SYS,echo=FALSE,fig.height=6,out.width='.95\\linewidth',fig.pos='th',fig.show = 'asis'>>=
  set.seed(43)
  N   <- 15
  n   <- 3
  ip  <- runif(N)
  ip  <- ip/sum(ip)*3
  V   <- cumsum(ip)
  rs  <- unique(ceiling(V))
  #pdf(file=paste0(graphPath,"presentation/presentation.pdf"), width = 10 , height = 10)
  par(mar=c(0, 0, 0, 0), xpd=NA)
   plot(c(0, 3), c(0, 3), type = "n", xlab = "", ylab = "", bty="n", axes=FALSE)
  lines(x=c(0,3),y=c(1.5,1.5),lwd=1.2)
  
  #integers
  arrows(x0=c(0,rs), y=1.5, x1=c(0,rs), y1=1.3, length=0, lwd=2)
    text(x=c(0, rs), y=1.2, labels=c(0,rs), cex=1.2, offset=0)
  
  #cumulated inclusion pobabilities
  arrows(x0=c(0,V) ,y=1.5, x1=c(0,V) , y1=1.6, length=0, lwd=1.5)
  text(x=0, y=1.7, labels=bquote(V^{.(0)}) , pos=1, offset=0, cex=1.2)
 
  for(i in 1:length(ip)){
   expr. <- bquote(V^{.(i)})
   text(x=V[i], y=1.7, labels=expr. ,cex=1.2, pos=1, offset=0)
  }
  
  #The sample 
  #S. <- UPsystematic.(n/N)
  u  <- runif(1)
  a = (c(0, cumsum( ip)) - u)%%1
  S. = as.integer(a[1:N] > a[2:(N + 1)])
  
  s.cod <- u + 0:(floor(max(V))-1)
  s.id <- which(S.==1)
  
  #graphPath1 <- paste0(graphPath,"SYS_coord_start_fixPop.pdf")
  #dev.copy(pdf,file=graphPath1)
  #dev.off()
 
  for(i in 1:length(s.id)){

  #graphPath2 <- paste0(graphPath,"SYS_coord_sample",i,".pdf")

   arrows(x0=s.cod[i] , y0=1.1, x1=s.cod[i], y1=1.5, length=0.1, lwd=1.5)
   if(i==1) text(x=s.cod[i]  , y=1, labels=bquote(lambda)  ,  cex=1.2)
   if(i>1)  text(x=s.cod[i]  , y=1, labels=bquote(lambda+.(i-1))  ,  cex=1.2)
     text(x=V[s.id[i]], y=1.7 , labels=bquote(V^{.(s.id[i])}), pos=1, offset=0
          ,cex=1.2, col="firebrick1")
   #dev.copy(pdf,file=graphPath2)
   #dev.off()
  }
  
@
~\\[-3cm]
Systematic selection remains popular because of its simplicity. Although unbiased variance estimation is in general not possible.
\end{frame}

\begin{frame}{Stratification}
A Population of 100 elements is stratified into $H=6$ strata.
<<StratSetUp,echo=FALSE>>=
N  <- 100
Xs <- rep(1:10,each=10)
Ys <- rep(1:10,time=10)

sind <- list(c(1:4,11:14,21:24)
             ,c(5:10,15:20,25:30)
             ,c(31:35,41:45,51:55)
             ,c(61:65,71:75,81:85,91:95)
             ,c(36:40,46:50,56:60,66:70)
             ,c(76:80,86:90,96:100))
#sind <- list((1:30),c(31:35,41:45,51:55),c(61:65,71:75,81:85,91:95))
SIND <- 1:100
SIND[sind[[1]]] <- 1
SIND[sind[[2]]] <- 2
SIND[sind[[3]]] <- 3
SIND[sind[[4]]] <- 4
SIND[sind[[5]]] <- 5
SIND[sind[[6]]] <- 6

strata.Xs <- split(Xs,SIND)
strata.Ys <- split(rev(Ys),SIND)
Nh <- sapply(sind,length)
n  <- 14
nh <- round(Nh/N*n)
@
%fig.keep='all',fig.show='asis'
\onslide*<1>{
<<StratPlot1,echo=FALSE,fig.width=12,out.width='.85\\linewidth'>>=
plot(  x=Xs
      ,y=Ys
      ,pch=20
      ,xlab=""
      ,ylab=""
      ,axes=FALSE
      ,mar=c(4,4,4,10)
      )
#stratification
rect(xleft=0  , ybottom = 0, xright = 3.5,ytop = 6.5,lwd=2)
text(x=2.5, y=6, labels = "h=2")

rect(xleft=0  , ybottom = 6.5, xright = 3.5,ytop = 11,lwd=2)
text(x=2.5, y=10, labels = "h=1")

rect(xleft=3.5, ybottom = 5.5, xright = 6.5, ytop = 11,lwd=2)
text(x=5.5, y=10, labels = "h=3")


rect(xleft=6.5, ybottom = 5.5, xright = 11, ytop = 11,lwd=2)
text(x=9.5, y=10, labels = "h=4")


rect(xleft=7.5, ybottom = 0, xright = 11, ytop = 5.5,lwd=2)
text(x=6.5, y=5, labels = "h=5")

text(x=9.5, y=5, labels = "h=6")

box(lwd=3)
@
}
\onslide<2>{
\Sexpr{n} elements are selected population and their allocation is given by
\begin{tabular}{cccccc}
$n_1$ = \Sexpr{nh[1]}  & $n_2$ = \Sexpr{nh[2]}  & $n_3$ = \Sexpr{nh[3]}  & $n_4$ = \Sexpr{nh[4]} & 
 $n_5$ = \Sexpr{nh[5]}&  $n_6$ = \Sexpr{nh[6]}\\
\end{tabular}

<<StratPlot2,echo=FALSE,fig.width=12,out.width='.85\\linewidth'>>=
<<StratPlot1>>
set.seed(947)
sam.str <- mapply(function(x,y)sample(x,y),Nh,nh)
for(i in 1:length(nh)) {
  points(x=strata.Xs[[i]][sam.str[[i]]],y=strata.Ys[[i]][sam.str[[i]]],pch=15,col=2,cex=2)
}

@
}
\end{frame}


% \begin{frame}{Issues with Stratification}
% \begin{itemize}
% \item<1-> Why should stratification be used?
% \begin{itemize}
% \item<2-> To reduce the sampling variance of estimators.
% \item<2-> Sometimes it is necessary because of organizational reasons, (e.g. no joint sampling frame).
% \end{itemize}
% \item<3-> How should the population be stratified?
% \begin{itemize}
% \item<4-> A \emph{good} set of variables needs to be found for stratification.
% \item<4-> The number of strata has to be decided.
% \end{itemize}
% \item<5-> How should the overall sample size be allocated to the strata?
% \begin{itemize}
% \item<6-> Achieve proportionality between sample and population (i.e. the frame)
% \item<6-> Fulfill precision constraints for certain estimation domains
% \end{itemize}
% \end{itemize}
% 
% \end{frame}

\begin{frame}{Defining the Strata}
  \begin{table}\caption{Population ANOVA}
  \begin{tabular}{l | l | l }
  Source & df & Sum of Squares  \\
  \hline
   Between strata         & $H-1$ & $\text{SSB}  = \sum_{h=1}^H N_h ( \mu_{h} - \mu  )^2$  \\
   Within  strata         & $N-H$ & $\text{SSW}  = \sum_{h=1}^H (N_h-1) V_{h}^2$  \\
   Total,  about  $\mu_y$ & $N-1$ & $\text{SSTO} = (N-1) V^2$ \\
  \end{tabular}
  \end{table}
Stratification can reduce the sampling variance of estimators. The more homogeneous the strata are the higher is the gain in efficiency from using a stratified sample sample instead of SRS. That is if the SSW (variance within) is
considerably smaller that than the SSB (variance between). 
\end{frame}

\begin{frame}{Allocation Methods}
For all $h = 1{,}\,\ldots{,}\,H$
 \begin{equation*} \arraycolsep=1.4pt\def\arraystretch{2.2}
  n_h = \left\{ \begin{array}{l r}
        \dfrac{n}{H}  &\; \text{equal allocation} \\
        \dfrac{N_h}{N}  n    &\; \text{proportional allocation} \\
        \dfrac{N_h V_{h}}{\sum_{h=1}^H N_h V_{h} }  n  &\; \text{optimal allocation} 
  \end{array}\;\right.{,}
 \end{equation*}

Proportional allocation can also be done with respect to another variable, e.g. $\dfrac{\tau_h}{\tau}n$

% Variants of proportional allocation to totals
% proportional to y or auxiliar variable
% %the rounding probelem
\end{frame}

\begin{frame}[fragile]{Example Stratification}
<<StratExam, echo=FALSE, cache=FASLE, results='hide', warning=FALSE, message=FALSE, cache=TRUE>>=
# library(survey)
# library(sampling)
# library(xtable) 
data(api)
cl <- kmeans(apipop[,c("api99","api00")], 10)
apipop$strat  <- cl$cluster

tv    <- mean(apipop$api00-apipop$api99)

rand.round <- function(x){
  
  int <- x%/%1
  round.int <- sampling::UPbrewer(x%%1)
  rannum <- runif(length(x))
  int+round.int
  
}


select.apiStra <- 
  function(n,straOpt=FALSE){
    #prob allocation
    
    dat <- apipop
 
    if(!straOpt){
      dat$strat <- dat$cnum
    }

    nh  <- rep(2, length(unique(dat$strat)))
    nh_ <- rand.round(table(dat$strat)/nrow(dat)*(n-sum(nh)))
    nh  <- nh+nh_
    
    s.snum <- unlist(
      mapply(function(x,y) sample(as.character(x), y), 
             tapply(dat$snum,dat$strat,identity)[names(nh)],
             nh
      )
    )
    
    dat$fpc1 <- table(dat$strat)[as.character(dat$strat)]
    apistra  <- dat[dat$snum %in% as.numeric(s.snum), ]
    
    
    svydesign(id=~snum, fpc = ~fpc1, strata = ~strat, data=apistra)
    
}

estdiffStra <- function(n,straOpt){

dclus_1 <- select.apiStra(n,straOpt=straOpt)
dclus_2 <- select.apiStra(n,straOpt=straOpt)

var.delta <- SE(svymean(~api00,dclus_1))^2 + SE(svymean(~api99,dclus_2))^2
delta     <- as.vector(svymean(~api00,dclus_2))-as.vector(svymean(~api99,dclus_1))

Conf.svy <- c(delta + qnorm(1-0.975)*sqrt(var.delta),
                 delta + qnorm(0.975)*sqrt(var.delta)
                 )
var.delta.naive <-
  svyvar(~api99,dclus_1 )[[1]]/nrow(dclus_1$variables)+
  svyvar(~api00,dclus_2 )[[1]]/nrow(dclus_2$variables)


Conf.naive <- c(delta + qnorm(1-0.975)*sqrt(var.delta.naive ),
                 delta + qnorm(0.975)*sqrt(var.delta.naive )
)

res.svy <- data.frame(t(c(delta, var.delta, Conf.svy)))
colnames(res.svy) <- c("Est","Vest","CI.lb","CI.ub")

res.naiv <- data.frame(t(c(delta, var.delta.naive, Conf.naive)))
colnames(res.naiv) <- c("Est","Vest","CI.lb","CI.ub")

rbind(Design=res.svy,Naive=res.naiv)
}

@
\onslide*<1>{
We would like to estimate the difference in the mean Academic Performance Index (API) of all Californian schools between year 1999 and 2000 ($\Sexpr{round(tv,2)}$). To do that we select from all Californian schools two samples. One sample in 1999 and one in 2000. Both samples are selected by a  stratified (simple random) sample, where the Counties of California are used as the strata. The samples size for both samples is 205. From each County at least 2 schools are selected. The rest of the sampled size is allocated proportionally to the number of schools in the strata. The inclusion probability of a school in a particular stratum is the number of schools selected from that stratum divided by the total number of schools in that stratum. 
}
\onslide*<2>{
We use two estimator for variance estimation. One is design unbiased and the other is a naive estimator that uses no other design information than the design weights ($ \hat\sigma^2/n$).

<<StratSamRes1,  echo=FALSE, results='asis'>>=
set.seed(1514)
res <- estdiffStra(205,straOpt=FALSE)
xtable(res,digits = c(0,2,3,3,3))
@

The stratification seems to be not very effective. So we construct 10 strata that are more homogeneous with regard to $\text{API}_{99}$ and $\text{API}_{00}$ (using \emph{k-means}) and select two new samples.

<<StratSamRes2,  echo=FALSE, results='asis'>>=
set.seed(1511)
res <- estdiffStra(205,straOpt=TRUE)
xtable(res,digits = c(0,2,3,3,3))

@
} 

\onslide*<3>{
We repeat the sampling with the better stratification 1000 times and compute the coverage rates for our confidence intervals.

<<StratSamRes3,  echo=FALSE, cache=TRUE, results='asis'>>=
set.seed(1512)

OUT <-
sapply(1:1000,function(x){

res <- estdiffStra(205,straOpt=TRUE)

svyH0    <- res["Design","CI.lb"] <= tv & res["Design","CI.ub"] > tv
naiveH0  <- res["Naive","CI.lb"] <=  tv & res["Naive","CI.ub"] > tv

c(svyH0, naiveH0)


})


res.sim <- data.frame(t(apply(OUT,1,mean)))
dimnames(res.sim) <- list("Coverage Rate",c("Design","Naive"))
xtable::xtable(res.sim,digits = c(0,3,3))
@
}

\end{frame}

\begin{frame}{Clustering}
<<CluSetUp,echo=FALSE>>=
N  <- 100
Xs <- rep(1:10,each=10)
Ys <- rep(1:10,time=10)

sind <- list(c(1:4,11:14,21:24)
             ,c(5:10,15:20,25:30)
             ,c(31:35,41:45,51:55)
             ,c(61:65,71:75,81:85,91:95)
             ,c(36:40,46:50,56:60,66:70)
             ,c(76:80,86:90,96:100))
#sind <- list((1:30),c(31:35,41:45,51:55),c(61:65,71:75,81:85,91:95))
SIND <- 1:100
SIND[sind[[1]]] <- 1
SIND[sind[[2]]] <- 2
SIND[sind[[3]]] <- 3
SIND[sind[[4]]] <- 4
SIND[sind[[5]]] <- 5
SIND[sind[[6]]] <- 6

strata.Xs <- split(Xs,SIND)
strata.Ys <- split(rev(Ys),SIND)
Nh <- sapply(sind,length)
n_I  <- 2
@
A Population of 100 elements is clustered into $N_{\RN{1}}=6$ cluster
%fig.keep='all',fig.show='asis'
\onslide*<1>{
<<CluPlot1,echo=FALSE,fig.width=12,out.width='.85\\linewidth'>>=
offset <- 0.65
plot(  x=Xs
      ,y=Ys
      ,pch=20
      ,xlab=""
      ,ylab=""
      ,axes=FALSE
      ,mar=c(4,4,4,10)
      )
rect(xleft=offset  , ybottom = offset, xright = 3.5,ytop = 6.5,lwd=2)
text(x=2.5, y=6, labels = "i=2")

rect(xleft=offset   , ybottom = 6.5, xright = 3.5,ytop = 11-offset,lwd=2)
text(x=2.5, y=10, labels = "i=1")

rect(xleft=3.5, ybottom = 5.5, xright = 6.5, ytop = 11-offset,lwd=2)
text(x=5.5, y=10, labels = "i=3")


rect(xleft=6.5, ybottom = 5.5, xright = 11-offset, ytop = 11-offset,lwd=2)
text(x=9.5, y=10, labels = "i=4")


rect(xleft=7.5, ybottom = offset, xright = 11-offset, ytop = 5.5,lwd=2)
text(x=6.5, y=5, labels = "i=5")

text(x=9.5, y=5, labels = "i=6")
box(lwd=3)

@
}\onslide<2>{
and $n_{\RN{1}}=\Sexpr{n_I}$ clusters are selected from the population.
<<CluPlot2,echo=FALSE,fig.width=12,out.width='.85\\linewidth'>>=
<<CluPlot1>>
set.seed(999)
sam.str <- sample(length(Nh),n_I)
par(xpd=TRUE)
if(any(sam.str==2))
 rect(xleft=offset  , ybottom = offset, xright = 3.5,ytop = 6.5,lwd=2, border=2) 
if(any(sam.str==1))
 rect(xleft=offset  , ybottom = 6.5, xright = 3.5,ytop = 11-offset,lwd=2,border=2)
if(any(sam.str==3))
 rect(xleft=3.5, ybottom = 5.5, xright = 6.5, ytop = 11-offset,lwd=2,border=2)
if(any(sam.str==4))
 rect(xleft=6.5, ybottom = 5.5, xright = 11-offset, ytop = 11-offset,lwd=2,border=2)
if(any(sam.str==5))
 rect(xleft=3.5, ybottom = offset, xright = 7.5, ytop = 5.5,lwd=2,border=2)
if(any(sam.str==6))
 rect(xleft=7.5, ybottom = offset, xright = 11-offset, ytop = 5.5,lwd=2,border=2)
par(xpd=FALSE)
for(i in 1:n_I) {
  points(x=strata.Xs[[sam.str[i]]],y=strata.Ys[[sam.str[i]]],pch=15,col=2,cex=2)
}

@
}
\end{frame}


\begin{frame}{Cluster Sampling}
\onslide<1->{ Sampling elementary units is often not feasible (e.g. persons or businesses). Maybe there is no uniform sampling frame available to select them from, or it would be costly to do, because the selected elements would scatter to much over the a certain area and travel costs of interviewers would be to high. 
}
\onslide<2->{Thus, it is very common to select clusters, so called \emph{primary sampling units} (PSU's) that are populated by \emph{secondary sampling units} (SSU's).
}
\onslide<3->{Cluster sampling makes it still possible to obtain unbiased estimates but it can have a big influence on the variance. 
}
\onslide<4->{Compared to stratification cluster sampling tends to increase the sampling variance. What makes stratification efficient, a small within variance, has the opposite effect on cluster sampling.
}
\end{frame}



\begin{frame}[fragile]{Example Clustering}
<<ClusExam, echo=FALSE, cache=FASLE, results='hide', warning=FALSE, message=FALSE, cache=TRUE>>=

data(api)

select.apiClu <- 
  function(n_I, UP){
    dat <- apipop

    if(UP){
    
    h_c  <- table(dat$dnum)/nrow(dat)*n_I
    
    
    p_c  <- h_c%%1
    d_c  <- h_c%/%1
    s_c  <- UPmaxentropy(p_c)
    
    n_i  <- s_c+d_c
    n_i[n_i>1] <- 1
    
    s.dnum <- names(h_c)[n_i>0]
    
    fpc1  <- h_c[as.character(dat$dnum)]
    fpc1[fpc1>1] <- 1
      
    dat$fpc1 <- fpc1
    
    
    apiclus <- dat[dat$dnum%in%as.numeric(s.dnum),]
    
    svydesign(id=~dnum, fpc = ~fpc1, data=apiclus, pps = "brewer")
    
    }else{
      
    s.dnum <- sample(as.character(unique(dat$dnum)), n_I)
        
    dat$fpc1 <- length(unique(dat$dnum))
    
    apiclus <- dat[dat$dnum %in% as.numeric(s.dnum), ]
    
    svydesign(id=~dnum, fpc = ~fpc1, data=apiclus)
      
    }
    
    
  }

estdiffClu <- function(n_I, UP){

dclus_1 <- select.apiClu(n_I, UP)
dclus_2 <- select.apiClu(n_I, UP)

var.delta <- SE(svymean(~api00,dclus_1))^2 + SE(svymean(~api99,dclus_2))^2
delta     <- as.vector(svymean(~api00,dclus_2))-as.vector(svymean(~api99,dclus_1))

Conf.svy <- c(delta + qnorm(1-0.975)*sqrt(var.delta),
                 delta + qnorm(0.975)*sqrt(var.delta)
                 )
var.delta.naive <-
  svyvar(~api99,dclus_1 )[[1]]/nrow(dclus_1$variables)+
  svyvar(~api00,dclus_2 )[[1]]/nrow(dclus_2$variables)


Conf.naive <- c(delta + qnorm(1-0.975)*sqrt(var.delta.naive ),
                 delta + qnorm(0.975)*sqrt(var.delta.naive )
)

res.svy <- data.frame(t(c(delta, var.delta, Conf.svy)))
colnames(res.svy) <- c("Est","Vest","CI.lb","CI.ub")

res.naiv <- data.frame(t(c(delta, var.delta.naive, Conf.naive)))
colnames(res.naiv) <- c("Est","Vest","CI.lb","CI.ub")

rbind(Design=res.svy,Naive=res.naiv)
}

@
\onslide*<1>{
Now we use for our Californian school survey cluster sampling. Both samples are selected by a (simple) cluster sample, where the clusters are the School Districts of California. 25 clusters are selected for both samples and the expected number of schools in each sample is $\Sexpr{round(mean(table(apipop$dnum))*25)}$. Each cluster has the same inclusion probability, $\Sexpr{25/length(unique(apipop$dnum))}$ (25 divided by $\Sexpr{length(unique(apipop$dnum))}$, the number of clusters). 
}
\onslide*<2>{
We use two estimator for variance estimation. One is design unbiased and the other is a naive estimator that uses no other design information than the design weights ($\hat\sigma^2/n$).

<<ClusSamRes1,  echo=FALSE, results='asis'>>=
set.seed(1528)
res <- estdiffClu(25, FALSE)
xtable(res,digits = c(0,2,3,3,3)) 
@

} 

\onslide*<3>{
We repeat the sampling 1000 times and compute the coverage rates for our confidence intervals.

<<ClusSamRes2,  echo=FALSE, cache=TRUE, results='asis'>>=
set.seed(1519)

OUT <-
sapply(1:1000,function(x){

res <-  estdiffClu(25, FALSE)

svyH0    <- res["Design","CI.lb"] <= tv & res["Design","CI.ub"] > tv
naiveH0  <- res["Naive","CI.lb"] <= tv & res["Naive","CI.ub"] > tv

c(svyH0, naiveH0)


})

res.sim <- data.frame(t(apply(OUT,1,mean)))
dimnames(res.sim) <- list("Coverage Rate",c("Design","Naive"))
xtable::xtable(res.sim,digits = c(0,3,3))
@


Because of the under estimation by the naive variance estimator the naive approach results in a severe under coverage.
The design based approach does not under estimate the variance but their is a problem with the application of the CLT for building the confidence intervals.

}

\onslide*<4>{
We repeat the simulation, but only with 100 replications and this time we sample the clusters proportional to their number of schools. Thus the inclusion probability of each cluster is $\dfrac{N_i}{N}*25$, where $N_i$ is the number of schools in the $i$-th cluster and N the total number of schools ($\Sexpr{nrow(apipop)}$).

<<ClusSamRes3,  echo=FALSE, cache=TRUE, results='asis'>>=
set.seed(1512)

OUT <-
sapply(1:100,function(x){

res <-  estdiffClu(25, TRUE)

svyH0    <- res["Design","CI.lb"] <= tv & res["Design","CI.ub"] > tv
naiveH0  <- res["Naive","CI.lb"] <= tv & res["Naive","CI.ub"] > tv

c(svyH0, naiveH0)


})

res.sim <- data.frame(t(apply(OUT,1,mean)))
dimnames(res.sim) <- list("Coverage Rate",c("Design","Naive"))
xtable(res.sim,digits = c(0,3,3))
@
}

\end{frame}



%\section{Complex Sampling Designs - Two Stage Sampling}

\begin{frame}{Two Stage Sampling}
% A Population of 100 elements is clustered into $N_{\RN{1}}=6$ clusters
% and $n_{\RN{1}}=\Sexpr{n_I}$ clusters (PSU) are selected at the first sampling stage
\onslide*<1>{
<<CluPlot1_2,echo=FALSE,fig.width=12,out.width='.85\\linewidth'>>=
set.seed(671)
n_i <- 4
sam.str <- sample(length(Nh),n_I)
#par(xpd=TRUE)
<<CluPlot1>>

if(any(sam.str==2))
rect(xleft=offset  , ybottom = offset, xright = 3.5,ytop = 6.5,lwd=2, border=2)

if(any(sam.str==1))
rect(xleft=offset  , ybottom = 6.5, xright = 3.5,ytop = 11-offset,lwd=2,border=2)


if(any(sam.str==3))
rect(xleft=3.5, ybottom = 5.5, xright = 6.5, ytop = 11-offset,lwd=2,border=2)


if(any(sam.str==4))
rect(xleft=6.5, ybottom = 5.5, xright = 11-offset, ytop = 11-offset,lwd=2,border=2)


if(any(sam.str==5))
rect(xleft=3.5, ybottom = offset, xright = 7.5, ytop = 5.5,lwd=2,border=2)


if(any(sam.str==6))
rect(xleft=7.5, ybottom = offset, xright = 11-offset, ytop = 5.5,lwd=2,border=2)
#par(xpd=FALSE)
@

}\onslide<2>{
and $n_{i}=4$ elements are selected from each sampled cluster.
<<CluPlot2_2,echo=FALSE,fig.width=12,out.width='.85\\linewidth'>>=
<<CluPlot1_2>>
for(i in sam.str) {
      s_i <- sample(length(strata.Xs[[i]]), n_i)
      points(x=strata.Xs[[i]][s_i],y=strata.Ys[[i]][s_i],pch=15,col=2,cex=2)
}
@
}





\end{frame}


\begin{frame}{Two Stage Sampling}
 \begin{description}
 \item[First  stage] A sample $\mathfrc{s}_{\RN{1}}$ of PSU's is drawn from $\mathcal{U}_{\RN{1}}$ according to some sampling design $p_{\RN{1}}(.)$
 \item[Second stage] For every $i \in \mathfrc{s}_{\RN{1}}$ a sample $\mathfrc{s}_i$ of SSU's is selected from $\mathcal{U}_i$ according to some design $p_i(.|\mathfrc{s}_{\RN{1}})$
 \end{description}
 The resulting sample of SSU's is denote $\mathfrc{s}= \bigcup_{i \in \mathfrc{s}_{\RN{1}}} \mathfrc{s}_i $.
 In general, samples $\mathfrc{s}_i$ are selected independently of each other, thus, the inclusion probability of a element $k \in \mathcal{U}_i$ is
$$\pi_k=\pi_{\RN{1}i}\pi_{k|i}\;,$$
where $\pi_{\RN{1}i}$ is the probability of selecting the $i$-th PSU and $\pi_{k|i}$  the probability of selecting
the $k$-th SSU within the $i$-th PSU.
\end{frame}



\begin{frame}[fragile]{Example Two Stage Sampling}
<<TwoStageExam, echo=FALSE, cache=FASLE, results='hide', warning=FALSE, message=FALSE, cache=TRUE>>=
set.seed(2251)
data(api)

select.apiTwoStage <- 
  function(n_I, n_c){
    dat <- apipop
    h_c  <- table(dat$cnum)/nrow(dat)*n_I
    
    
    p_c  <- h_c%%1
    d_c  <- h_c%/%1
    s_c  <- UPmaxentropy(p_c)
    
    n_i  <- (s_c+d_c)*n_c
    
    s.snum <- unlist(
      mapply(function(x,y) sample(as.character(x), y), 
             tapply(dat$snum,dat$cnum,identity)[names(n_i)],
             n_i
      )
    )
    
    h_ci  <- h_c[as.character(dat$cnum)]
    n_i.m <- rep(n_c,length(h_ci))
    
    n_i.m[h_ci>1] <- h_ci[h_ci>1]*n_c
    
    p_ci         <- h_ci
    p_ci[h_ci>1] <- 1
    
    dat$fpc2 <- n_i.m[dat$cnum]/table(dat$cnum)[as.character(dat$cnum)]
    dat$fpc1 <- p_ci[as.character(dat$cnum)]
    
    
    apiclus3 <- dat[dat$snum%in%as.numeric(s.snum),]
    
    svydesign(id=~cnum+snum, fpc = ~fpc1 + fpc2, data=apiclus3, pps = "brewer")
    
    
    
  }

dmultstag_1 <- select.apiTwoStage(25,2)

sum.lm.naive <- summary(lm(api00~ell+meals+mobility+stype, 
                           data = dmultstag_1$variables, 
                           weights = weights(dmultstag_1)))

sum.lm.svy   <- summary(svyglm(api00~ell+meals+mobility+stype, 
                               design=dmultstag_1))

@
\onslide*<1>{
For our Californian schools would like to estimate the following model $\text{API}_{00} = \text{ell}+\text{meals}+\text{mobility}+\text{stype}$,
where
\begin{itemize}
\item[ell] = English Language Learners (percent)
\item[meals] =  Percentage of students eligible for subsidized meals,
\item[mobility] = percentage of students for whom this is the first year at the school,
\item[stype] = Elementary/Middle/High School
\end{itemize}
Now we use a two stage sample. As PSUs the counties of California are used, the SSU are the schools. 25 PSUs are selected with probablity proportional to their number of schools. Within each selected PSU 2 schools are sampled by a SRS.
 }
\onslide<2>{

<<TwoStageRes,  echo=FALSE, cache=FALSE, results='asis'>>=
tab.naive<-xtable(sum.lm.naive,digits = c(0,3,3,3,3),caption = "Naive")
tab.svy  <- xtable(sum.lm.svy,digits = c(0,3,3,3,3),caption = "Design")

print(tab.naive, scalebox=0.6,caption.placement='top')
print(tab.svy, scalebox=0.6,caption.placement='top')
@
}
\end{frame}


\section{Sample Size Planning}

\begin{frame}{Selecting a Sample Size}
\onslide*<1>{
The sample size can be set to achieve a desired level of precision in terms of the variance 
$\V{\hat{\theta}}$ or the variation coefficient $\text{CV}(\hat{\theta})  = \frac{\sqrt{\text{V}(\hat{\theta})}}{\hat{\theta}}$. \newline
Set $\text{CV}(\overline{y}) = \text{CV}_0$ as a precision requirement (representative!).
\begin{align}
n & = \dfrac{ V^2  \mu^{-2}}{\text{CV}_0^2 +  V^2  N^{-1} \mu^{-2}} \eqname{SRS} 
 \end{align}
}
\onslide*<2>{
There are many ways to optimize the sampling design with
respect to one particular goal, i.e. the estimation of a specific
statistic. However, it becomes difficult to optimize a design and at
the same time retain a balance for a maximum of possible
applications, which is a problem when planning a multipurpose
survey that has a multitude of variables and covers different
topics. Thus simple design, such as SRS or stratified SRS, are
justifiable, as these designs are robust towards any possible
analysis of the sample data.
}
\end{frame}

\begin{frame}{Sample Size for Proportions}
\onslide*<1,3>{If the variable of interest is binary we have $\V{\overline{y}}_{\text{SRS}} = \dfrac{\mu(1-\mu) }{n} \dfrac{N-n}{N-1}$ and $\text{CV}^2(\overline{y})_\text{SRS}=\left( \dfrac{1}{n} - \dfrac{1}{N}\right) \dfrac{N}{N-1} \dfrac{(1-\mu)}{\mu}$. However $ \lim_{\mu \to 0} \text{CV}^2(\overline{y})_\text{SRS} = \infty\;$, thus for rare observation to meet a CV target the sample size can become very large.
}
\onslide*<2>{
<<SamSizeSRSprobs,echo=FALSE,fig.width=12,fig.height=8,out.width='.85\\linewidth',message=FALSE,warning=FALSE,fig.cap="Sample Sizes to Achieve CV's of 0.05 and 0.01 for N=50000">>=
 CV_01 <- 0.05
 CV_02 <- 0.01
 p <- seq(0.001, 0.999, length.out = 100)
 N <- 50000

 n_01 <- nProp(CV0 = CV_01,pU = p,N = N)
 n_02 <- nProp(CV0 = CV_02,pU = p,N = N)
 
 plot(x=p ,y=n_01,type="l",lwd=3,ylab="n",panel.first = grid(lwd=3),cex.lab=2, cex.axis=2,ylim=c(range(n_02)),xlab=expression(mu) )
 lines(x=p,y=n_02,lwd=3, type="l", lty=2)
 legend(0.7, max(n_02), c("CV=0.05","CV=0.01"), cex=2, lty=1:2,lwd=2)
@
}
\onslide*<3>{The target for $\V{\overline{y}}_{\text{SRS}}$ can be set to achieve a CI's with a maximal length of $2\epsilon$.
% \begin{align*}
% \epsilon \geq & z_{1-\alpha/2} \sqrt{ \dfrac{\mu(1-\mu) }{n} \dfrac{N-n}{N-1} } &
% n \geq & \dfrac{ z_{1-\alpha/2}^2 \frac{N}{N-1} \mu(1-\mu) }{\epsilon^2  + \frac{1}{N}  z_{1-\alpha/2}^2 \frac{N}{N-1} \mu(1-\mu) }
% \end{align*}
}
\onslide*<4>{
 <<SamSizeSRSprobs2,echo=FALSE,fig.width=12,fig.height=8,out.width='.85\\linewidth',message=FALSE,warning=FALSE,fig.cap="Sample Sizes to Achieve Absolute Errors of 0.05 and 0.01 for N=50000">>=
 e_01 <- 0.05
 e_02 <- 0.01
 p <- seq(0.001, 0.999, length.out = 100)
 N <- 50000
 z <- qnorm(1-0.05/2)
 n_01 <- (N/(N-1)*z^2*p*(1-p))/(e_01^2+z^2*p*(1-p)/(N-1))
 n_02 <- (N/(N-1)*z^2*p*(1-p))/(e_02^2+z^2*p*(1-p)/(N-1))
 
 plot(x=p ,y=n_02,type="l",lwd=3,ylab="n",panel.first = grid(lwd=3),cex.lab=2, cex.axis=2,lty=2,xlab=expression(mu) )
 lines(x=p,y=n_01,lwd=3, type="l", lty=1)
 legend(0.75, max(n_02), c(expression(epsilon~"=0.05"),expression(epsilon~"=0.01")), cex=2, lty=1:2,lwd=2)
@
}
\end{frame}

\begin{frame}[allowframebreaks]\frametitle{Literature}    
%\scriptsize
  \begin{thebibliography}{10}    
   \setbeamertemplate{bibliography item}[book]
   \bibitem{Lohr1999}
    S.~Lohr.
    \newblock  Sampling: Design and Analysis.
    \newblock {\em Duxbury Press}, 1999.
   \setbeamertemplate{bibliography item}[book]
  \bibitem{Lumley2010}
    T.~Lumley.
    \newblock Complex Surveys: A Guide to Analysis Using R.
    \newblock {\em Wiley}, 2010.
   \setbeamertemplate{bibliography item}[book]
  \bibitem{Saerndal1992}
    C.-E.~S\"{a}rndal, B.~Swensson, \& J.~Wretman.
    \newblock Model Assisted Survey Sampling
    \newblock {\em Springer}, 1992.
    \setbeamertemplate{bibliography item}[book]
  \bibitem{Tille2006}
   Y.~Till\'{e}.
  \newblock  Sampling Algorithms
    \newblock {\em Springer Series in Statistics: Springer}, 2006.
  \end{thebibliography}
\end{frame} 


\end{document} 